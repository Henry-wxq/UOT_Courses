\documentclass[fontsize=12pt]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage{latexsym, amsfonts, fullpage, lscape, cancel, array, lastpage}
\usepackage{mathrsfs, graphicx, amssymb, amsmath, amscd, amsthm, MnSymbol}
\usepackage{titlesec}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{float}
\usepackage{appendix}
\usepackage{color}
\usepackage{amsmath}
\usepackage{pifont}
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{amssymb}

\titleformat{\section}
  {\normalfont\Large\bfseries}{\S\thesection}{1em}{}

\usepackage[english]{babel}
\usepackage{eucal}

\title{Unit 1(a) Lecture Notes for MAT224}
\author{Xuanqi Wei 1009353209}

\date{\today}

\begin{document}

\maketitle

\noindent
These notes are intended for study purposes. You should be able to fill in these blanks from the notes you take during lecture and/or the textbook. You are welcome to use them to work ahead. Your completed copy of these notes should be submitted to the Quercus assignment called ``Unit 1(a) Lecture Notes'' by April 7, 2023. You can scan your handwritten answers or you can type them out. See the Unit 1 Homework for details.

\newpage

\section{Vector Spaces 1.1}

\textcolor{cyan}{What is a vector space?}\\
\\
A vector space is a collection of vectors that can be added together and multiplied (`scaled') by numbers, called scalars, in a consistent way. Formally, a vector space is defined as a set $V$ of vectors over a field $\mathbb{F}$, where vector addition and scalar multiplication satisfy the following axioms:

\begin{enumerate}
	\item Commutativity: $\mathbf{u}+\mathbf{v}=\mathbf{v}+\mathbf{u}$ for all $\mathbf{u},\mathbf{v}\in V$.
	\item Associativity: $(\mathbf{u}+\mathbf{v})+\mathbf{w}=\mathbf{u}+(\mathbf{v}+\mathbf{w})$ and $a(b\mathbf{u})=(ab)\mathbf{u}$ for all $\mathbf{u},\mathbf{v},\mathbf{w}\in V$ and $a,b\in\mathbb{F}$.
	\item Identity: There exists a vector $\mathbf{0}\in V$ such that $\mathbf{u}+\mathbf{0}=\mathbf{u}$ for all $\mathbf{u}\in V$.
	\item Inverse: For each $\mathbf{u}\in V$, there exists a vector $-\mathbf{u}\in V$ such that $\mathbf{u}+(-\mathbf{u})=\mathbf{0}$.
	\item Distributivity: $a(\mathbf{u}+\mathbf{v})=a\mathbf{u}+a\mathbf{v}$ and $(a+b)\mathbf{u}=a\mathbf{u}+b\mathbf{u}$ for all $\mathbf{u},\mathbf{v}\in V$ and $a,b\in\mathbb{F}$.
	\item Scalar multiplication identity: $1\mathbf{u}=\mathbf{u}$ for all $\mathbf{u}\in V$.
\end{enumerate}

\noindent
\textcolor{cyan}{Give three examples of vector spaces that are not $\mathbb{R}^2$, $\mathbb{R}^3$, or $\mathbb{R}^n$ for any $n \in \mathbb{N}$.}

The space of $n \times n$ complex matrices, denoted by $M_{n \times n}(\mathbb{C})$. Here, the vectors are $n \times n$ complex matrices, and addition and scalar multiplication are defined in the usual way. The dimension of this vector space is $n^2$.

The space of all polynomials with real coefficients of degree at most $n$, denoted by $\mathbb{R}_n[x]$. Here, the vectors are polynomials $a_0+a_1x+\cdots+a_nx^n$, and addition and scalar multiplication are defined in the usual way. The dimension of this vector space is $n+1$.

The space of all continuous functions from $[0,1]$ to $\mathbb{R}$, denoted by $C([0,1])$. Here, the vectors are functions $f : [0,1] \to \mathbb{R}$ that are continuous, and addition and scalar multiplication are defined pointwise. The dimension of this vector space is infinite, and it is an example of an infinite-dimensional vector space.

\newpage

\section{Subspaces 1.2}

\textcolor{cyan}{What is the subspace criterion? What does it say?}\\

The subspace criterion is a test that determines whether a subset of a vector space is itself a vector space. In other words, it gives conditions under which a subset of a vector space inherits the vector space structure from the original space. The subspace criterion states that a subset $U$ of a vector space $V$ is a subspace of $V$ if and only if the following three conditions hold:

\begin{enumerate}
	\item $\mathbf{0}_V\in U$: the zero vector of $V$ is also in $U$.
	\item Closure under vector addition: if $\mathbf{u},\mathbf{v}\in U$, then $\mathbf{u}+\mathbf{v}\in U$.
	\item Closure under scalar multiplication: if $\mathbf{u}\in U$ and $a$ is a scalar, then $a\mathbf{u}\in U$.
\end{enumerate}

Here, $\mathbf{0}_V$ denotes the zero vector of the vector space $V$. Condition 1 ensures that $U$ contains the zero vector, which is a necessary condition for it to be a vector space. Conditions 2 and 3 ensure that $U$ is closed under vector addition and scalar multiplication, respectively, which are the defining operations of a vector space.
\\

\noindent 
\textcolor{cyan}{What is another definition of a subspace that is NOT the subspace criterion?}\\

A subset $U$ of a vector space $V$ is a subspace of $V$ if and only if $U$ is non-empty and closed under linear combinations.

Here, a linear combination of vectors in $U$ is any expression of the form $a_1\mathbf{u}_1+a_2\mathbf{u}_2+\cdots+a_k\mathbf{u}_k$, where $\mathbf{u}_1,\mathbf{u}_2,\ldots,\mathbf{u}_k$ are vectors in $U$ and $a_1,a_2,\ldots,a_k$ are scalars.

The condition of being non-empty means that $U$ must contain at least one vector. The condition of being closed under linear combinations means that any linear combination of vectors in $U$ must also be in $U$. In other words, $U$ must be closed under linear combinations.
\\

\noindent
\textcolor{cyan}{Give an example of a subspace of $M_{2 \times 2}(\mathbb{R})$. Prove it is a subspace.}\\

Let $W$ be the set of all $2\times 2$ symmetric matrices over $\mathbb{R}$, that is,

$$W = \left\{\left(\begin{array}{cc}a & b\\b & c  \end{array} \right) \in M_{2\times2}(\mathbb{R}) \right\}.$$

We claim that $W$ is a subspace of $M_{2 \times 2}(\mathbb{R})$.

To prove that $W$ is a subspace, we need to verify the two conditions:

\begin{enumerate}
	\item $W$ is non-empty: This is clear, since the zero matrix is in $W$.
	\item $W$ is closed under linear combinations: Let $A,B$ be two matrices in $W$, and let $k_1,k_2$ be scalars. We need to show that $k_1A+k_2B$ is also in $W$.
\end{enumerate}

$$k_1 + k_2 = \left( \begin{array}{cc} k_1a+k_2a & k_1b+k_2b \\k_1b+k_2b & k_1c+k_2c \end{array} \right) = \left( \begin{array}{cc} (k_1+k_2)a & (k_1+k_2)b \\(k_1+k_2)b & (k_1+k_2)c \end{array} \right)$$
\\

which is a symmetric matrix. Therefore, $k_1A+k_2B\in W$, and $W$ is closed under linear combinations.

Since $W$ satisfies the two conditions for being a subspace, we conclude that $W$ is a subspace of $M_{2 \times 2}(\mathbb{R})$.
\\

\noindent
\textcolor{cyan}{Give an example of a subset $S$ of a vector space $V$ that is not a subspace. Explain why it is not a subspace.}\\

Consider the set $S={(x,y)\in \mathbb{R}^2 : x+y=1}$, which is a subset of the vector space $\mathbb{R}^2$.

We claim that $S$ is not a subspace of $\mathbb{R}^2$.

To see why $S$ is not a subspace, we need to check one of the conditions of the subspace criterion that fails. Here, we see that $S$ fails to satisfy the closure under vector addition property, which is the second condition of the subspace criterion.

To see this, consider the vectors $\mathbf{u}=(1,0)$ and $\mathbf{v}=(0,1)$, which are both in $S$ since $1+0=0+1=1$. However, their sum $\mathbf{u}+\mathbf{v}=(1,1)$ is not in $S$, since $1+1=2\neq 1$. Therefore, $S$ is not closed under vector addition and hence not a subspace of $\mathbb{R}^2$.

\section{Linear Independence 1.3}

\noindent
\textbf{Definition of Linear Independence:} The set of vectors $\{ v_1, v_2, \ldots v_n\}$ is linearly independent if and only if the only solution to the equation $c_1v_1+c_2v_2+\ldots + c_nv_n = 0$ is the trivial solution $c_1=c_2=\cdots=c_n=\mathbf{0}$, where $\mathbf{0}$ is the zero vector.\\

\noindent
\textbf{Definition of Linear Dependence:} The set of vectors $\{ v_1, v_2, \ldots v_n\}$ is linearly dependent if and only if there exist scalars $c_1, c_2, \ldots, c_n$, not all zero, such that the equation $c_1=c_2=\cdots=c_n=\mathbf{0}$ has a nontrivial solution, where $\mathbf{0}$ is the zero vector. \\

\noindent
\textcolor{cyan}{Can a set of only one vector be linearly dependent? Why or why not?}

A set containing only one vector cannot be linearly dependent, as there is no other vector to compare it to. This is because linear dependence refers to the property of a set of vectors such that at least one vector in the set can be written as a linear combination of the other vectors in the set.

${\textbf{v}}$ is linearly independent if and only if $c_1\textbf{v} = \textbf{0}$ implies $c_1 = 0$, for all $c_1 \in \mathbb{R}$ and $\textbf{v} \in \mathbb{R}^n$.
\\

\noindent
\textcolor{cyan}{Can a set of 5 vectors in $\mathbb{R}^4$ be linearly independent?}\\
Let $B = \{ v_1, v_2, v_3, v_4, v_5\}, \ v_i \in \mathbb{R}^4, \ i = 1, 2, \ldots 5$. Then $B$ is not linearly independent. This is the consequence of the Steinitz exchange lemma, which states that for a vector space V, if L is a linearly independent set and S is a spanning set, then we have $\|L\| \leq \|S\|$. Since we may take S to be a basis of V, this implies that we must have $\|L\| \leq dim(V)$.

\noindent
Similarly, In this example, $\|B\| \geq dim(V)$, therefore, a set of 5 vectors can't be linearly independent in $\mathbb{R}^4$.

\newpage

\noindent
\textcolor{cyan}{Give an example of a proof that a set is linearly independent.}
\\
Let $v_1 = \begin{pmatrix} 1 \ 0 \ 2 \end{pmatrix}, v_2 = \begin{pmatrix} 0 \ 1 \ -1 \end{pmatrix}, v_3 = \begin{pmatrix} 2 \ -1 \ 3 \end{pmatrix}$ be three vectors in $\mathbb{R}^3$. We want to show that the set $S = {v_1, v_2, v_3}$ is linearly independent.

\noindent
Suppose that there exist scalars $c_1, c_2, c_3$ such that $c_1v_1+c_2v_2+c_3v_3 = \left(\begin{array}{c} 0 \\ 0 \\ 0
\end{array} \right).$

\noindent
Then we have the system of linear equations: $$\left\{ \begin{array}{rcl} c_1 + 2c_3 = 0 \\ c_2 - c_3 = 0 \\ 2c_1 - c_2 + 3c_3 = 0 \end{array} \right.$$

\noindent
Since the system has only the trivial solution $c_1 = c_2 = c_3 = 0$, we can conclude that $S$ is linearly independent.

\noindent
Therefore, the set $S = {v_1, v_2, v_3}$ is linearly independent.\\


\noindent
\textcolor{cyan}{Give an example of a proof that a set is linearly dependent.}

\noindent
Let $v_1 = \begin{pmatrix} 1 \ 2 \ 3 \end{pmatrix}, v_2 = \begin{pmatrix} -1 \ 0 \ 1 \end{pmatrix}, v_3 = \begin{pmatrix} 2 \ 4 \ 6 \end{pmatrix}$ be three vectors in $\mathbb{R}^3$. We want to show that the set $S = {v_1, v_2, v_3}$ is linearly dependent.

\noindent
We notice that $v_3 = 2v_1 + v_2$. Indeed, we have

$$2v_1 + v_2 - v_3 = 2\cdot \left(\begin{array}{c} 1 \\ 2 \\ 3 \end{array} \right) + \left(\begin{array}{c} -1 \\ 0 \\1 \end{array} \right) = \left(\begin{array}{c} 1 \\ 4 \\7 \end{array} \right) = v_3$$
\noindent
Therefore, the set $S$ is linearly dependent, since we have found a nontrivial linear combination of $v_1$, $v_2$, and $v_3$ that equals the zero vector:

$$2v_1 + v_2 - v_3 = 2\cdot \left(\begin{array}{c} 1 \\ 2 \\ 3 \end{array} \right) + \left(\begin{array}{c} -1 \\ 0 \\1 \end{array} \right) - \left(\begin{array}{c} 2 \\ 4 \\6 \end{array} \right) = \left(\begin{array}{c} 0 \\ 0 \\0 \end{array} \right) = v_3$$

\noindent
\textcolor{cyan}{Which of the above two examples is also an example of disproving that a set is linearly independent? Why?}

\noindent
The second example is also an example of disproving that a set is linearly independent, because it shows that the set $S = {v_1, v_2, v_3}$ is not linearly independent by providing a nontrivial linear combination of $v_1$, $v_2$, and $v_3$ that equals the zero vector. This means that there exist scalars $c_1$, $c_2$, and $c_3$, not all zero, such that

$$c_1v_1+c_2v_2+c_3v_3 = \left(\begin{array}{c} 0 \\ 0 \\ 0
\end{array} \right).$$

which is the definition of linearly dependent. Specifically, we have shown that $2v_1 + v_2 - v_3 = \mathbf{0}$, so $S$ is linearly dependent.

\end{document}
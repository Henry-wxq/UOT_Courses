\documentclass[fontsize=12pt]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage{latexsym, amsfonts, fullpage, lscape, cancel, array, lastpage}
\usepackage{mathrsfs, graphicx, amssymb, amsmath, amscd, amsthm, MnSymbol}
\usepackage{titlesec}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{float}
\usepackage{appendix}
\usepackage{color}
\usepackage{amsmath}
\usepackage{pifont}
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{amssymb}

\titleformat{\section}
  {\normalfont\Large\bfseries}{\S\thesection}{1em}{}

\usepackage[english]{babel}
\usepackage{eucal}

\title{Unit 2(a) Lecture Notes for MAT224}
\author{Xuanqi Wei 1009353209}

\date{24-26 January 2023}

\begin{document}

\maketitle

\newpage

\section{1.6 Bases and Dimension Continued...}

\textcolor{cyan}{Write two definitions for a set $S$ to be a basis for a vector space $V$.}

\begin{enumerate}
	\item A set $S$ is a basis for a vector space $V$ if and only if every element of $V$ can be expressed as a linear combination of the vectors in $S$, and $S$ is linearly independent.
	\item A set $S$ is a basis for a vector space $V$ if and only if $S$ spans $V$ and $S$ is linearly independent.
\end{enumerate}
\\

\noindent
\textcolor{cyan}{Consider the statements of theorems: 1.6.3, 1.6.6, and 1.6.10, 1.6.14, 1.6.18, Lemma 1.6.8, and Corollary 1.6.11.}\\
\\
\textcolor{cyan}{How much can they tell you about the answers to the following questions? Let $V = span\{s_1, s_2, \ldots , s_n\}$ for some $n \in \mathbb{N}$} \\
\\
\begin{itemize}
    \item[(a)] \textcolor{cyan}{Does $V$ have at least one basis?} \\
    \\ Yes, $V$ has at least one basis. This is because any spanning set can be reduced to a basis by removing any linearly dependent vectors. The resulting set will still span $V$ and will be linearly independent, making it a basis for $V$. If the original set is already linearly independent, then it is already a basis for $V$. Therefore, every vector space has at least one basis.
    \\
    \item[(b)] \textcolor{cyan}{Do all bases of a given vector space have the same number of elements?}\\
    \\ Yes, all bases of a given vector space have the same number of elements, which is called the dimension of the vector space. This is a fundamental result in linear algebra known as the dimension theorem.
    \\
    \item[(c)] \textcolor{cyan}{If a subspace $W$ of vector space $V$ has a basis, can that basis be extended (have vector(s) added to it) to a basis for all of $V$?}\\
    \\ Yes, it is always possible to extend a basis of a subspace $W$ to a basis of the larger vector space $V$. This is known as the "basis extension theorem" or `Steinitz exchange lemma'. The basic idea behind the theorem is that we can take any linearly independent set of vectors in $V$ that is not already in the span of the basis of $W$, and then replace some of the vectors in the basis of $W$ with the new linearly independent vectors to get a new basis for $V$. The theorem provides a systematic way to make this replacement while still maintaining linear independence and ensuring that the resulting set spans all of $V$.
    \\
    \item[(d)] \textcolor{cyan}{Does it make more sense to talk about \textbf{a} dimension of vector space $V$ or \textbf{the} dimension of vector space $V$? In other words, is there more than one candidate for the value of $dim(V)$?}
    \\ It makes more sense to talk about `the' dimension of vector space $V$. The dimension of a vector space is a well-defined concept, and it is unique for a given vector space. While different bases of $V$ may have different numbers of vectors, they will all have the same number of vectors, which is the dimension of $V$. Therefore, we can talk about `the' dimension of $V$ rather than `a' dimension.
    \end{itemize}
    
\newpage

\section{2.1 Linear Transformations}

\textcolor{cyan}{Give two definitions for ``linear transformation''.}

\begin{enumerate}
	\item A linear transformation is a function $T: V \rightarrow W$ between two vector spaces $V$ and $W$ that preserves the operations of addition and scalar multiplication. In other words, for any vectors $\mathbf{u}, \mathbf{v} \in V$ and any scalar $c \in \mathbb{R}$, the following properties hold:
\begin{enumerate}
	\item $T(\mathbf{u}+\mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v})$
	\item $T(c\mathbf{u}) = cT(\mathbf{u})$
\end{enumerate}
	\item A linear transformation is a function $T: V \rightarrow W$ that satisfies the following two properties for any vectors $\mathbf{u}, \mathbf{v} \in V$ and any scalars $c, d \in \mathbb{R}$:
\begin{enumerate}
	\item $T(\mathbf{u}+\mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v})$
	\item $T(c\mathbf{u}+d\mathbf{v}) = cT(\mathbf{u}) + dT(\mathbf{v})$
\end{enumerate}
\end{enumerate}

\noindent
\textcolor{cyan}{Is it possible to determine the pre-image of a vector $\vec{w} \in W$, if you know its image? In other words, let $T: V \to W$ be a linear transformation and let $\vec{w} \in T(W)$. So $T(v) = w$ for some $v \in V$. Can we tell which $v \in V$ is sent to $W$ by V.}

\noindent
Not necessarily. If $\vec{w}$ is in the range of $T$, then there exists at least one vector $\vec{v} \in V$ such that $T(\vec{v})=\vec{w}$. However, there may be other vectors in $V$ that are also mapped to $\vec{w}$ by $T$. In fact, if $T$ is not injective (i.e., if there exist distinct vectors $\vec{v_1}, \vec{v_2} \in V$ such that $T(\vec{v_1})=T(\vec{v_2})=\vec{w}$), then there are infinitely many vectors in $V$ that are mapped to $\vec{w}$ by $T$. Therefore, without additional information about $T$ or $\vec{w}$, we cannot determine the pre-image of $\vec{w}$ with certainty.
\\

\noindent
\textcolor{cyan}{Consider Proposition 2.1.14. It suggests that, if we know the value of $T(b_i)$ for every $b_1, b_2, \ldots , b_n$ in a basis $B$ of $V$ and linear transformation $T:V \to W$, then we can find the value of $T(v)$  for any $v \in V$. Explain how we would do this.}\\
\\
\textcolor{cyan}{For example, if $T(1) = 5$, $T(x) = 1 +2x$, and $T(x^2) = 1 - 2x + 3x^2$, what is the value of $T(2x+4)$?}

\noindent
To find the value of $T(v)$ for any $v \in V$, we first express $v$ as a linear combination of the basis vectors $b_1, b_2, \ldots , b_n$ of $V$. That is, we write $v = c_1 b_1 + c_2 b_2 + \cdots + c_n b_n$ for some scalars $c_1, c_2, \ldots , c_n$.

\noindent
Then, we can use the linearity of $T$ to compute $T(v)$. Specifically, we have:

$$T(v) = T(c_1b_1 + c_2b_2 + \ldots + c_nb_n) = c_1T(b_1) + c_2T(b_2) + \ldots + c_nT(b_n) $$

\noindent
So, to find the value of $T(v)$ for any $v \in V$, we simply need to know the values of $T(b_i)$ for every $b_i$ in a basis $B$ of $V$.

\noindent
In the given example, let $B = {1,x,x^2}$ be a basis of $V$. We are given that $T(1) = 5$, $T(x) = 1+2x$, and $T(x^2) = 1-2x+3x^2$. To find $T(2x+4)$, we express $2x+4$ as a linear combination of the basis vectors in $B$:

$$2X+4 = 2X + 0\cdot 1 + 0 \cdot x^2 = 2x \cdot 1 + 0 \cdot (1+2x) + 0 \cdot (1-2x + 3x^2) $$
Then, we use the linearity of $T$ to compute $T(2x+4)$:

\begin{align*}
T(2x+4) &= T(2x\cdot 1 + 0\cdot(1+2x) + 0\cdot(1-2x+3x^2)) \\
&= 2xT(1) + 0\cdot T(x) + 0\cdot T(x^2) \\
&= 2x\cdot 5 + 0\cdot (1+2x) + 0\cdot (1-2x+3x^2) \\
&= 10
\end{align*}

Therefore, we have $T(2x+4) = 10$.

\section{2.2 Linear Transformations Between Finite Dimensional Vector Spaces}

\noindent
\textcolor{cyan}{Give three examples of linear transformations $T: V_1 \to V_2$.}

\begin{enumerate}
	\item Let $V_1$ and $V_2$ be Euclidean spaces of dimension $n$ and $m$, respectively, and let $A$ be an $m \times n$ matrix. Define $T:\mathbb{R}^n \to \mathbb{R}^m$ by $T(\mathbf{x})=A\mathbf{x}$. This is an example of a linear transformation because it satisfies the two properties of additivity and homogeneity: $T(\mathbf{x}+\mathbf{y}) = A(\mathbf{x}+\mathbf{y}) = A\mathbf{x} + A\mathbf{y} = T(\mathbf{x})+T(\mathbf{y})$ and $T(k\mathbf{x}) = A(k\mathbf{x}) = kA\mathbf{x} = kT(\mathbf{x})$ for all vectors $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$ and scalars $k$.
	\item Let $V_1$ and $V_2$ be the spaces of polynomials of degree at most 2 and 3, respectively. Define $T:V_1 \to V_2$ by $T(p(x)) = xp(x)$, where $p(x) = a_0 + a_1x + a_2x^2$ is a polynomial in $V_1$. This is a linear transformation because it satisfies additivity and homogeneity: $T(p(x) + q(x)) = x(p(x) + q(x)) = xp(x) + xq(x) = T(p(x)) + T(q(x))$ and $T(kp(x)) = kxp(x) = kT(p(x))$ for all polynomials $p(x), q(x) \in V_1$ and scalars $k$.
	\item Let $V_1$ and $V_2$ be the spaces of continuous functions on $[0,1]$ and $[1,2]$, respectively. Define $T:V_1 \to V_2$ by $T(f(x)) = f(x+1)$, where $f(x)$ is a function in $V_1$. This is a linear transformation because it satisfies additivity and homogeneity: $T(f(x) + g(x)) = (f(x) + g(x) + 1) = f(x+1) + g(x+1) = T(f(x)) + T(g(x))$ and $T(kf(x)) = kf(x+1) = kT(f(x))$ for all functions $f(x), g(x) \in V_1$ and scalars $k$.
\end{enumerate}
\\

\noindent
\textcolor{cyan}{Give an example of a vector space $V$, two bases $\alpha = \{ a_1, a_2, \ldots , a_n\}$ and $\beta = \{ b_1, b_2, \dots , b_n\}$ of $V$, a transformation $T: V\to V$, and its matrix forms $\left[T\right]_{\alpha}^{\beta}$ and $\left[T\right]_{\beta}^{\alpha}$.}

\noindent
Let $V$ be the vector space $\mathbb{R}^2$ over the field $\mathbb{R}$, and let $\alpha = {a_1, a_2}$ and $\beta = {b_1, b_2}$ be two bases of $V$ defined as follows:

$$a_1 = \left(\begin{array}{c} 1 \\ 0 \end{array} \right); a_2 = \left(\begin{array}{c} 1 \\ 1 \end{array} \right); b_1 = \left(\begin{array}{c} 2 \\ 1 \end{array} \right); b_2 = \left(\begin{array}{c} -1 \\ 2 \end{array} \right)$$

\noindent
Let $T: V \rightarrow V$ be a linear transformation defined by $T(x,y) = (x+y, y-x)$ for all $(x,y) \in V$. We want to find the matrix representations $\left[T\right]{\alpha}^{\beta}$ and $\left[T\right]{\beta}^{\alpha}$ of $T$ with respect to the bases $\alpha$ and $\beta$.

\noindent
To find $\left[T\right]{\alpha}^{\beta}$, we first find the images of the basis vectors under $T$:

$$T(a_1) = T\left(\begin{array}{c} 1 \\ 0 \end{array} \right) = \left(\begin{array}{c} 1 \\ 0 \end{array} \right) = 1\cdot \left(\begin{array}{c} 2 \\ 1 \end{array} \right) + 0 \cdot \left(\begin{array}{c} -1 \\ 2 \end{array} \right) = 1b_1 + 0b_2$$

$$T(a_2) = T\left(\begin{array}{c} 1 \\ 1 \end{array} \right) = \left(\begin{array}{c} 2 \\ 0 \end{array} \right) = 1\cdot \left(\begin{array}{c} 2 \\ 1 \end{array} \right) + (-1) \cdot \left(\begin{array}{c} -1 \\ 2 \end{array} \right) = 1b_1 - 1b_2$$

Therefore, $\left[T\right]_{\alpha}^{\beta} = \begin{pmatrix} 1 & 1 \\ 0 & -1 \end{pmatrix}$.

To find $\left[T\right]_{\beta}^{\alpha}$, we first find the pre-images of the basis vectors under $T$. Solving the equations $T(x,y) = b_1$ and $T(x,y) = b_2$, we get:

$$T^{-1}(b_1) = \left(\begin{array}{c} \frac{3}{2} \\ \frac{1}{2} \end{array} \right)\ and \ T^{-1}(b_2) = \left(\begin{array}{c} \frac{1}{2} \\ \frac{3}{2} \end{array} \right)$$

respectively. Therefore,

$$\left[T\right]_{\beta}^{\alpha} = (T^{-1}(b_1) \ \ T^{-1}(b_2)) = \begin{pmatrix} \frac{3}{2} & \frac{1}{2} \\ \frac{1}{2} & \frac{3}{2} \end{pmatrix} $$

\noindent
\textcolor{cyan}{What is the process of getting from a linear transformation $T$ to its matrix form $\left[T\right]_{\alpha}^{\beta}$? Is it possible to go backwards and determine $T$ from its matrix?}

\noindent
To obtain the matrix form $\left[T\right]_{\alpha}^{\beta}$ of a linear transformation $T: V \to W$ with respect to bases $\alpha$ and $\beta$ of $V$ and $W$ respectively, we follow these steps:

\begin{enumerate}
	\item For each basis vector $a_i$ in $\alpha$, we apply $T$ to it and express the result as a linear combination of the vectors in $\beta$. This gives us a column vector $[![T(a_i)]!]{\beta}$ in the matrix $\left[T\right]{\alpha}^{\beta}$.
	\item We arrange the column vectors obtained in step 1 side by side to form the matrix $\left[T\right]_{\alpha}^{\beta}$.
\end{enumerate}

\noindent
To go backwards and determine $T$ from its matrix $\left[T\right]_{\alpha}^{\beta}$, we use the following steps:

\begin{enumerate}
	\item We choose a basis $\alpha$ for the domain $V$ and a basis $\beta$ for the codomain $W$.
	\item We use the matrix $\left[T\right]{\alpha}^{\beta}$ to define a function $T{\alpha}^{\beta}: \mathbb{R}^n \to \mathbb{R}^m$ where $n$ is the dimension of $V$ and $m$ is the dimension of $W$. This function takes a column vector in $\mathbb{R}^n$ as input and outputs a column vector in $\mathbb{R}^m$ obtained by multiplying the matrix $\left[T\right]_{\alpha}^{\beta}$ by the input column vector.
	\item We define the function $T: V \to W$ by setting $T(a_i) = \sum_{j=1}^m \left[T\right]_{\alpha_j}^{\beta_i} w_j$, where $\alpha_j$ is the $j$th basis vector in $\alpha$ and $w_j$ is the $j$th basis vector in $\beta$.
\end{enumerate}

\end{document}

\documentclass[fontsize=12pt]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage{latexsym, amsfonts, fullpage, lscape, cancel, array, lastpage}
\usepackage{mathrsfs, graphicx, amssymb, amsmath, amscd, amsthm, MnSymbol}
\usepackage{titlesec}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{float}
\usepackage{appendix}
\usepackage{color}
\usepackage{amsmath}
\usepackage{pifont}
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{amssymb}

\titleformat{\section}
  {\normalfont\Large\bfseries}{\S\thesection}{1em}{}

\usepackage[english]{babel}
\usepackage{eucal}

\title{Unit 3(a) Lecture Notes for MAT224}
\author{Xuanqi Wei 1009353209}

\date{7-9 February 2023}

\begin{document}

\maketitle

\newpage

\section{2.5 Composition of Linear Transformations}

\bigskip

\noindent
\textcolor{cyan}{Let $S:U \to V$ and $T: V \to W$ be linear transformations. Prove that $TS$ is also a linear transformation. What is its domain? What is its codomain?}\\

\noindent
To show that $TS$ is a linear transformation, we need to show that it satisfies the two properties of linearity:
\begin{enumerate}
	\item Additivity: $(TS)(\mathbf{u}+\mathbf{v}) = (TS)(\mathbf{u}) + (TS)(\mathbf{v})$ for all vectors $\mathbf{u}, \mathbf{v} \in U$.
	\item Homogeneity: $(TS)(a\mathbf{u}) = a(TS)(\mathbf{u})$ for all scalars $a$ and vectors $\mathbf{u} \in U$.
\end{enumerate}
\noindent
Let $\mathbf{u},\mathbf{v} \in U$ and $a \in \mathbb{R}$. Then we have:
\begin{align*}
(TS)(\mathbf{u}+\mathbf{v}) &= T(S(\mathbf{u}+\mathbf{v})) && \text{(definition of composition)}\\
&= T(S(\mathbf{u}) + S(\mathbf{v})) && \text{(linearity of }S\text{)}\\
&= T(S(\mathbf{u})) + T(S(\mathbf{v})) && \text{(linearity of }T\text{)}\\
&= (TS)(\mathbf{u}) + (TS)(\mathbf{v}) && \text{(definition of composition)}
\end{align*}
\noindent
Therefore, $TS$ satisfies the additivity property. Similarly, we have:

\begin{align*}
(TS)(a\mathbf{u}) &= T(S(a\mathbf{u})) && \text{(definition of composition)}\\
&= T(aS(\mathbf{u})) && \text{(linearity of }S\text{)}\\
&= aT(S(\mathbf{u})) && \text{(linearity of }T\text{)}\\
&= a(TS)(\mathbf{u}) && \text{(definition of composition)}
\end{align*}
\noindent
Therefore, $TS$ satisfies the homogeneity property. Hence, $TS$ is a linear transformation.

\noindent
The domain of $TS$ is $U$, since the domain of $S$ is $U$ and $T$ takes as input the output of $S$, which is a vector in $V$. The codomain of $TS$ is $W$, since $T$ maps vectors in $V$ to vectors in $W$.

\newpage

\noindent
\textcolor{cyan}{Consider linear transformations $S: U \to V$ and $T: V \to W$.\\
We say that $A \subset B$ if $\forall a \in A$ we can show that $a \in B$ as well.}\\
\\
(Proposition 2.5.6) Prove that:\\
\indent \textcolor{cyan}{(i) $Ker(S) \subset Ker(TS)$}\\
To prove that $Ker(S) \subset Ker(TS)$, we need to show that if a vector $\mathbf{u}$ is in $Ker(S)$, then it is also in $Ker(TS)$.

\noindent
Suppose $\mathbf{u} \in Ker(S)$. Then by definition, we have $S(\mathbf{u}) = \mathbf{0}_V$, where $\mathbf{0}_V$ is the zero vector in $V$. Now consider the vector $(TS)(\mathbf{u})$. By the definition of composition, we have:
$$(TS)(u) = T(S(u)) = T(0_V) = 0_W $$

\noindent
where the last equality follows from the fact that $T$ is a linear transformation and maps the zero vector in $V$ to the zero vector in $W$. Therefore, we have shown that if $\mathbf{u} \in Ker(S)$, then $(TS)(\mathbf{u}) = \mathbf{0}_W$, which means that $\mathbf{u} \in Ker(TS)$.

\noindent
Hence, we have shown that $Ker(S) \subset Ker(TS)$.

\indent \textcolor{cyan}{(ii) $Im(TS) \subset Im(T)$}\\
\noindent
To prove that $Im(TS) \subset Im(T)$, we need to show that every vector in the image of $TS$ is also in the image of $T$. That is, if $\mathbf{w} \in Im(TS)$, we need to show that $\mathbf{w} \in Im(T)$.

\noindent
Since $\mathbf{w} \in Im(TS)$, there exists a vector $\mathbf{u} \in U$ such that $(TS)(\mathbf{u}) = \mathbf{w}$. Now consider the vector $S(\mathbf{u}) \in V$. Since $T$ maps vectors in $V$ to vectors in $W$, we have $T(S(\mathbf{u})) \in Im(T)$. But we also know that $TS(\mathbf{u}) = T(S(\mathbf{u}))$, so we have:

$$w = TS(u) = T(S(u)) \in Im(T) $$

\noindent
Therefore, we have shown that if $\mathbf{w} \in Im(TS)$, then $\mathbf{w} \in Im(T)$, which implies that $Im(TS) \subset Im(T)$.

\vspace{4.5 in}

\noindent
\textcolor{cyan}{Why does it not make sense to compare $Ker(T)$ and $Ker(TS)$?}\\
\noindent
It does not make sense to compare $Ker(T)$ and $Ker(TS)$ because they are not necessarily related in any meaningful way.

\noindent
Recall that the kernel (also known as the null space) of a linear transformation is the set of all vectors that the transformation maps to the zero vector. In other words, $Ker(T)$ is the set of all vectors $\mathbf{v}$ in the domain of $T$ such that $T(\mathbf{v}) = \mathbf{0}$.

\noindent
On the other hand, $Ker(TS)$ is the set of all vectors $\mathbf{u}$ in the domain of $S$ such that $(TS)(\mathbf{u}) = \mathbf{0}$. Note that $Ker(TS)$ involves both $S$ and $T$, while $Ker(T)$ only involves $T$.

\noindent
While it is true that $Ker(T) \subset Ker(TS)$ (as we showed in a previous answer), this does not necessarily mean that $Ker(T)$ and $Ker(TS)$ are comparable in any other way. In general, the dimensions and properties of $Ker(T)$ and $Ker(TS)$ can be very different, and it is not meaningful to compare them without additional information.
\\

\noindent
\textcolor{cyan}{Similarly, why does it not make sense to compare $Im(S)$ and $Im(TS)$?}\\
\noindent
It does not make sense to compare $Im(S)$ and $Im(TS)$ in general because they are not necessarily related in any meaningful way.

\noindent
Recall that the image (also known as the range) of a linear transformation is the set of all vectors in the codomain that can be obtained by applying the transformation to vectors in the domain. In other words, $Im(S)$ is the set of all vectors in $V$ that can be written as $S(\mathbf{u})$ for some vector $\mathbf{u}$ in the domain of $S$. Similarly, $Im(TS)$ is the set of all vectors in $W$ that can be written as $(TS)(\mathbf{u})$ for some vector $\mathbf{u}$ in the domain of $S$.

\noindent
While it is true that $Im(S) \subset Im(TS)$, this does not necessarily mean that $Im(S)$ and $Im(TS)$ are comparable in any other way. In general, the dimensions and properties of $Im(S)$ and $Im(TS)$ can be very different, and it is not meaningful to compare them without additional information.

\newpage

\noindent
\textcolor{cyan}{If $W$ is a subspace of $V$, what do we know about the dimensions of $W$ and $V$? How does this relate to Corollary 2.5.7?}\\
\\
Corollary 2.5.7 says that:
\begin{itemize}
    \item[(i)] $dim(Ker(S)) \leq dim(Ker(TS))$
    \item[(ii)] $dim(Im(TS)) \leq dim(Im(T))$
\end{itemize}
\noindent
If $W$ is a subspace of $V$, then we know that the dimension of $W$ is less than or equal to the dimension of $V$, since every basis of $W$ is a linearly independent subset of $V$ and hence can be extended to form a basis of $V$.

\noindent
Corollary 2.5.7 says that if $S: U \to V$ and $T: V \to W$ are linear transformations, then $dim(Ker(S)) \leq dim(Ker(TS))$ and $dim(Im(TS)) \leq dim(Im(T))$. These inequalities relate to the rank-nullity theorem, which says that for any linear transformation $T: U \to V$, we have:

$$dim(Ker(T))+ dim(Ker(TS)) = dim(U) $$

\noindent
Using this theorem, we can see that $dim(Im(TS)) = dim(V) - dim(Ker(TS))$ and $dim(Im(T)) = dim(W) - dim(Ker(T))$. Thus, the inequality $dim(Im(TS)) \leq dim(Im(T))$ can be rewritten as:

$$dim(V) - dim(Ker(TS)) \leq dim(W) - dim(Ker(T)) $$
$$dim(Ker(T)) \leq dim(Ker(TS)) $$

\noindent
which is the first inequality in Corollary 2.5.7.

\noindent
Therefore, if $W$ is a subspace of $V$, then we can apply Corollary 2.5.7 to the linear transformations $S: {0} \to W$ and $T: W \to V$, which gives us:

$$dim(Ker(S)) \leq dim(Ker(TS))\ and\ dim(Im(TS)) \leq dim(Im(T)) $$

\noindent
Since $S$ is the zero transformation from the trivial vector space ${0}$ to $W$, we have $Ker(S) = {0}$ and $dim(Ker(S)) = 0$. Therefore, the first inequality simplifies to:
$$0\leq dim(Ker(T)) $$

\noindent
which is always true. The second inequality simplifies to:

$$dim(W) \leq dim(Im(T)) $$

\noindent
which is true since $W$ is a subspace of $V$ and hence $Im(T)$ contains $W$ as a subspace. Therefore, we have shown that if $W$ is a subspace of $V$, then Corollary 2.5.7 holds for the linear transformations $S: {0} \to W$ and $T: W \to V$.

\newpage

\noindent
\textcolor{cyan}{Consider the matrices $[T]_{\alpha}^{\beta}$,  $[S]_{\gamma}^{\delta}$, and $[TS]_{\epsilon}^{\zeta}$. Which of bases $\alpha$, $\beta$, $\gamma$, $\delta$, $\epsilon$, and $\zeta$ have to be equal to each other in order for the following equation to be true: $$[T]_{\alpha}^{\beta} [S]_{\gamma}^{\delta} = [TS]_{\epsilon}^{\zeta}$$}
\noindent
For the equation $[T]_{\alpha}^{\beta} [S]_{\gamma}^{\delta} = [TS]_{\epsilon}^{\zeta}$ to be true, the bases $\beta$ and $\gamma$ must be equal, since the number of columns of $[T]_{\alpha}^{\beta}$ must equal the number of rows of $[S]_{\gamma}^{\delta}$.

\noindent
Moreover, the basis $\epsilon$ of $TS$ must be the same as the basis $\alpha$ of $T$, since the left-hand side of the equation has basis $\alpha$ and the right-hand side has basis $\epsilon$. Therefore, we must have $\epsilon = \alpha$.

\noindent
Finally, the basis $\zeta$ of $TS$ must be the same as the basis $\delta$ of $S$, since the number of columns of $[TS]_{\epsilon}^{\zeta}$ must equal the number of columns of $[S]_{\gamma}^{\delta}$. Therefore, we must have $\zeta = \delta$.

\noindent
In summary, for the equation $[T]_{\alpha}^{\beta} [S]_{\gamma}^{\delta} = [TS]_{\epsilon}^{\zeta}$ to be true, we must have $\beta = \gamma$, $\epsilon = \alpha$, and $\zeta = \delta$.

\noindent
\textcolor{cyan}{In what ways are Propositions 2.5.4 and 2.5.14 similar? How are they different?}
\noindent
Proposition 2.5.4 and Proposition 2.5.14 are both about linear transformations, but they are different in their statements and conclusions.

\noindent
Proposition 2.5.4 states that if $T: V \to W$ is a linear transformation, then $T$ is injective if and only if $\operatorname{Ker}(T) = {0}$. In other words, $T$ maps distinct vectors in $V$ to distinct vectors in $W$ if and only if the only vector in $V$ that is mapped to $0$ is the zero vector.

\noindent
On the other hand, Proposition 2.5.14 states that if $T: V \to W$ is a linear transformation, then $T$ is surjective if and only if $\operatorname{Im}(T) = W$. In other words, for every vector $w \in W$, there exists at least one vector $v \in V$ such that $T(v) = w$.

\noindent
So while both propositions are about linear transformations, they address different properties of such transformations. Proposition 2.5.4 is concerned with injectivity, which is a property related to the kernel of the transformation, while Proposition 2.5.14 is concerned with surjectivity, which is a property related to the image of the transformation.

\newpage

\section{2.6 Inverses of a Linear Transformation and Isomorphisms}

\bigskip

\noindent
\textcolor{cyan}{Consider linear transformation $T: V \to W$, where $V$ and $W$ are finite dimensional vector spaces.}\\
\\
What does it mean for $T$ to be:
\begin{itemize}
    \item[(i)] injective: For a linear transformation $T: V \to W$ to be injective, it means that for any two distinct vectors $v_1$ and $v_2$ in $V$, their images under $T$ are also distinct in $W$. Formally, $T$ is injective if and only if for any $v_1, v_2 \in V$, if $T(v_1) = T(v_2)$, then $v_1 = v_2$.
    \item[(ii)] surjective: For a linear transformation $T: V \to W$ to be surjective, it means that every vector in $W$ is the image of at least one vector in $V$ under $T$. Formally, $T$ is surjective if and only if for any $w \in W$, there exists at least one $v \in V$ such that $T(v) = w$.
    \item[(iii)] invertible: A linear transformation $T: V \to W$ is invertible if and only if there exists a linear transformation $T^{-1}: W \to V$ such that $T \circ T^{-1} = \operatorname{id}_W$ and $T^{-1} \circ T = \operatorname{id}_V$, where $\operatorname{id}_V$ and $\operatorname{id}_W$ are the identity transformations on $V$ and $W$, respectively.
\end{itemize}

\medskip

\noindent
\textcolor{cyan}{Also, how are these concepts related?}
\noindent
The concepts of injectivity, surjectivity, and invertibility are all related to the idea of how a linear transformation $T: V \to W$ maps vectors in $V$ to vectors in $W$.

\noindent
Injectivity, or one-to-one-ness, means that each vector in $V$ is mapped to a unique vector in $W$. In other words, no two distinct vectors in $V$ are mapped to the same vector in $W$. Geometrically, this means that the linear transformation does not "squish" any vectors in $V$ to the same point in $W$.

\noindent
Surjectivity, or onto-ness, means that every vector in $W$ is the image of at least one vector in $V$. Geometrically, this means that every point in $W$ is "hit" by at least one vector in $V$, or equivalently, that the image of $T$ spans all of $W$.

\noindent
Invertibility means that there is a way to "undo" the effect of the linear transformation $T$ on any vector in $V$ by applying $T^{-1}$, and vice versa. This means that the linear transformation $T$ is both injective and surjective, since invertibility implies that no two distinct vectors in $V$ are mapped to the same vector in $W$ and that every vector in $W$ is the image of at least one vector in $V$. Geometrically, invertibility means that the linear transformation does not "squish" or "fold" any vectors in $V$, but instead preserves the full geometric structure of $V$ in $W$.

\newpage
\noindent
\textcolor{cyan}{Show that the inverse transformation of a bijection is also a linear transformation. In other words, if $T$ is a bijective linear transformation, show that $T^{-1}$ is also linear.}
\noindent
To show that the inverse transformation of a bijection is also a linear transformation, we need to verify that $T^{-1}$ satisfies the two properties of linearity:
\begin{enumerate}
	\item $T^{-1}(u+v) = T^{-1}(u) + T^{-1}(v)$ for all vectors $u, v$ in the codomain of $T^{-1}$
	\item $T^{-1}(cu) = c T^{-1}(u)$ for all vectors $u$ in the codomain of $T^{-1}$ and all scalars $c$.
\end{enumerate}
\noindent
To prove property 1, let $u$ and $v$ be vectors in the codomain of $T^{-1}$, so that $T(T^{-1}(u)) = u$ and $T(T^{-1}(v)) = v$. Since $T$ is bijective, it is invertible, so we can apply $T^{-1}$ to both sides of the equation $T(T^{-1}(u+v)) = u+v$ to get $T^{-1}(u+v) = T^{-1}(u) + T^{-1}(v)$, as desired.

\noindent
To prove property 2, let $u$ be a vector in the codomain of $T^{-1}$ and let $c$ be a scalar. Again, since $T$ is bijective and invertible, we can apply $T^{-1}$ to both sides of the equation $T(T^{-1}(cu)) = cu$ to get $T^{-1}(cu) = cT^{-1}(u)$, as desired.

\noindent
Therefore, $T^{-1}$ satisfies the two properties of linearity, and is therefore a linear transformation.

\noindent
\textcolor{cyan}{Two vector spaces $V$ and $W$ are said to be isomophic if we can define a bijection between them. In this situation, we call the bijection and ``isomorphism". Define an isomorphism between the vector space of polynomials of degree at most 4, $P_3(\mathbb{R}$ and the vector space of $2 \times 2$ matrices $M_{2 \times 2}(\mathbb{R})$.}
\noindent
To define an isomorphism between $P_3(\mathbb{R})$ and $M_{2 \times 2}(\mathbb{R})$, we need to find a bijective linear transformation $T: P_3(\mathbb{R}) \to M_{2 \times 2}(\mathbb{R})$.

\noindent
Let ${1, x, x^2, x^3}$ be a basis for $P_3(\mathbb{R})$ and let ${A_{11}, A_{12}, A_{21}, A_{22}}$ be the standard basis for $M_{2 \times 2}(\mathbb{R})$. We can define a linear transformation $T: P_3(\mathbb{R}) \to M_{2 \times 2}(\mathbb{R})$ as follows:

$$T(a + bx+ cx^2 + dx^3) = \left(\begin{array}{cc} a & b \\ c & d \end{array} \right)$$
\noindent
In other words, we map a polynomial of degree at most 3 to a $2 \times 2$ matrix by setting the top-left entry to the constant coefficient, the top-right entry to the coefficient of $x$, the bottom-left entry to the coefficient of $x^2$, and the bottom-right entry to the coefficient of $x^3$.

\noindent
To show that $T$ is a bijection, we need to show that it is both injective and surjective.

\noindent
To show that $T$ is injective, we need to show that if $T(p) = T(q)$ for two polynomials $p$ and $q$ in $P_3(\mathbb{R})$, then $p=q$. Suppose that $T(p) = T(q)$, i.e., that the matrices corresponding to $p$ and $q$ are equal. Then we have:

$$ \left(\begin{array}{cc} a_p & b_p \\ c_p & d_p \end{array} \right) = \left(\begin{array}{cc} a_q & b_q \\ c_q & d_q \end{array} \right)$$
\noindent
This implies that $a_p = a_q$, $b_p = b_q$, $c_p = c_q$, and $d_p = d_q$. But these coefficients uniquely determine the polynomials $p$ and $q$, so we must have $p = q$, and hence $T$ is injective.

\noindent
To show that $T$ is surjective, we need to show that for every $2 \times 2$ matrix $A$ in $M_{2 \times 2}(\mathbb{R})$, there exists a polynomial $p$ in $P_3(\mathbb{R})$ such that $T(p) = A$. This is straightforward: given a matrix $A$, we can construct a polynomial $p$ of degree at most 3 whose coefficients are the entries of $A$ in the appropriate order, and then we have $T(p) = A$. Hence $T$ is surjective.

\noindent
Therefore, $T$ is a bijective linear transformation between $P_3(\mathbb{R})$ and $M_{2 \times 2}(\mathbb{R})$, and hence $P_3(\mathbb{R})$ and $M_{2 \times 2}(\mathbb{R})$ are isomorphic.
\\

\noindent
\textcolor{cyan}{What must be true of the dimensions of two vectors space $V$ and $W$ if they are isomorphic to each other?}

\noindent
If two vector spaces $V$ and $W$ are isomorphic, then they must have the same dimension. This is because an isomorphism between $V$ and $W$ is a bijective linear transformation, which preserves the dimension of the vector space. Specifically, if $T: V \rightarrow W$ is an isomorphism, then for any basis $\beta$ of $V$, the set $T(\beta)$ is a basis of $W$, and since bases have the same size, $V$ and $W$ must have the same dimension.

\newpage

\noindent
\textcolor{cyan}{Proposition 2.6.11 States that if $T: V \to W$ is an isomorphism between finite-dimensional vector spaces, then for any choice of bases $\alpha$ for $V$ and $\beta$ for W,}\\
\\
\textcolor{cyan}{$$[T^{-1}]_{\beta}^{\alpha} = \left( [T]_{\alpha}^{\beta}\right)^{-1}$$}

\bigskip

\noindent
\textcolor{cyan}{Confirm that this is true when $T$ is the derivative, $V = \{ ax^2 + bx \ | \ a,b \in \mathbb{R} \}$ with basis $\{x, x^2\}$, and $W = \{mx + b \ | \ m,b \in \mathbb{R} \}$ with basis $\{1, x\}$.}\\
\noindent
Let $T: V \rightarrow W$ be the derivative operator, where $V = { ax^2 + bx \ | \ a,b \in \mathbb{R} }$ with basis ${x, x^2}$, and $W = {mx + b \ | \ m,b \in \mathbb{R} }$ with basis ${1, x}$.

\noindent
First, let's find the matrix representation of $T$ with respect to these bases. For any $ax^2 + bx \in V$, we have:
$$T(ax^2 + bx) = \frac{d}{dx}(ax^2 + bx) = 2ax + b $$

\noindent
So the matrix representation of $T$ with respect to the basis ${x, x^2}$ for $V$ and the basis ${1, x}$ for $W$ is:
$$[T]_{\alpha}^{\beta} = \left(\begin{array}{cc} 0 & 1 \\ 2 & 0 \end{array} \right) $$
\noindent
where $\alpha = {x, x^2}$ and $\beta = {1, x}$.

\noindent
To find the inverse of this matrix, we can use the formula:
$$(A^{-1})_{i, j} = \frac{(-1)^{i, j}}{det(A)}det(A_{j, i}) $$
\noindent
where $A_{j,i}$ is the $(n-1) \times (n-1)$ matrix obtained by deleting the $i$th row and $j$th column of $A$. In this case, we have:
$$\left(\begin{array}{cc} 0 & 1 \\ 2 & 0 \end{array} \right)^{-1} = \frac{1}{(0)(0) - (1)(2)}\left(\begin{array}{cc} 0 & -1 \\ -2 & 0 \end{array} \right) = -\frac{1}{2}\left(\begin{array}{cc} 0 & -1 \\ -2 & 0 \end{array} \right) = \left(\begin{array}{cc} 0 & \frac{1}{2} \\ -1 & 0 \end{array} \right) $$

\noindent
Now, we can find $[T^{-1}]_{\beta}^{\alpha}$ using the formula from Proposition 2.6.11:

$$[T^{-1}]_{\beta}^{\alpha} = ([T]_{\alpha}^{\beta})^{-1} = \left(\begin{array}{cc} 0 & \frac{1}{2} \\ -1 & 0 \end{array} \right) $$

\noindent
Therefore, we have confirmed that Proposition 2.6.11 holds for this particular example.

\newpage

\noindent
\textcolor{cyan}{Why is it okay to talk about $T^{-1}$ here, even though the derivative in general does not have an inverse function?}

\noindent
In this case, we are not talking about the inverse function of the derivative as a function from $\mathbb{R}$ to $\mathbb{R}$. Instead, we are considering the inverse function of the linear transformation $T:V \to W$ that is defined as $T(p(x)) = p'(x)$, where $V$ is the vector space of polynomials of degree at most 2, and $W$ is the vector space of polynomials of degree at most 1.

\noindent
Since $T$ is a linear transformation and is bijective, it has an inverse transformation $T^{-1}: W \to V$, which is also a linear transformation. We can use this inverse transformation to calculate the matrix representation of $T^{-1}$ in terms of the chosen bases for $V$ and $W$, as given by Proposition 2.6.11.
\newpage


\section{2.7 Change of Basis}

\bigskip

\noindent
\textcolor{cyan}{What is a change of basis matrix? Give an example, including the two bases.}
\noindent
A change of basis matrix is a matrix that allows us to represent the same vector with respect to different bases.

\noindent
Suppose we have a vector space $V$ with two bases $\alpha = {v_1, v_2}$ and $\beta = {w_1, w_2}$. Let $P$ be the matrix whose $i$th column is the coordinate vector of $w_i$ with respect to the basis $\alpha$. Then $P$ is the change of basis matrix from $\alpha$ to $\beta$.

\noindent
To be more specific, let's consider the following example:

\noindent
Let $V$ be the vector space of polynomials of degree at most $2$ with real coefficients. Let $\alpha = {1, x, x^2}$ and $\beta = {1 + x, x - 1, 2 - x^2}$ be two bases for $V$. We want to find the change of basis matrix from $\alpha$ to $\beta$, which we'll call $P$. To do this, we find the coordinate vectors of the vectors in $\beta$ with respect to $\alpha$ and form them into a matrix.
\begin{align*}
[1 + x]{\alpha} &= \begin{bmatrix}1 \\ 1 \\ 0\end{bmatrix}, &
[x - 1]{\alpha} &= \begin{bmatrix}-1 \\ 1 \\ 0\end{bmatrix}, &
[2 - x^2]_{\alpha} &= \begin{bmatrix}2 \\ 0 \\ -1\end{bmatrix}.
\end{align*}

Thus, the change of basis matrix from $\alpha$ to $\beta$ is

$$P = \begin{bmatrix}1 & -1 & 2 \\ 1 & 1 & 0 \\ 0 & 0 & -1\end{bmatrix} $$

\noindent
This matrix tells us how to take the coordinate vector of a polynomial with respect to $\alpha$ and convert it to a coordinate vector with respect to $\beta$.
\\

\newpage

\noindent
\textcolor{cyan}{Consider $[I]_{\alpha}^{\beta}$ and vector $[v]_{\alpha}$. Show why the product $[I]_{\alpha}^{\beta}[v]_{\alpha}$ produces $[v]_{\beta}$, the vector whose elements are the coefficients when we write vector $v$ as a linear combination of elements in basis $\beta$.}

\noindent
Let $\alpha = {v_1, v_2, ..., v_n}$ be a basis for the vector space $V$, and $\beta = {w_1, w_2, ..., w_n}$ be another basis for $V$. Let $I$ be the identity transformation from $V$ to $V$, and let $[I]_{\alpha}^{\beta}$ be the change of basis matrix from $\alpha$ to $\beta$.

\noindent
Now, let $v$ be a vector in $V$, and let $[v]{\alpha}$ be the coordinate vector of $v$ with respect to $\alpha$. That is, $[v]{\alpha} = \begin{bmatrix} a_1 \ a_2 \ \vdots \ a_n \end{bmatrix}$, where $v = a_1 v_1 + a_2 v_2 + \cdots + a_n v_n$.

\noindent
We want to show that $[I]{\alpha}^{\beta}[v]{\alpha} = [v]{\beta}$. To do this, we first need to find the coordinate vector $[v]{\beta}$ of $v$ with respect to $\beta$.

\noindent
Since $\alpha$ and $\beta$ are both bases for $V$, we can write $v$ as a linear combination of elements in $\beta$:

$$v = b_1w_1 + b_2w_2 + \ldots + b_nw_n $$

\noindent
for some scalars $b_1, b_2, \dots, b_n$. To find these scalars, we can use the fact that $[I]_{\alpha}^{\beta}$ is the change of basis matrix from $\alpha$ to $\beta$. In other words, for any vector $u$ in $V$, we have:

$$[u]_{\beta} = [I]_{\alpha}^{\beta}[u]_{\alpha} $$
\noindent
Applying this to $v$, we get:
$$[v]_{\beta} = [I]_{\alpha}^{\beta}[v]_{\alpha} $$

\noindent
Therefore, $[I]{\alpha}^{\beta}[v]{\alpha}$ produces $[v]_{\beta}$, the vector whose elements are the coefficients when we write vector $v$ as a linear combination of elements in basis $\beta$.

\newpage


\noindent
\textcolor{cyan}{What has to be true of linear transformation $T$ in order for $[T]_{\alpha}^{\beta}$ to equal the identity $n \times n$ matrix which looks like:}

$$I_{n \times n}\left( \begin{array}{cccc} 1 & 0 & \ldots & 0\\
0 & 1 & \ldots & 0\\
\vdots & \vdots &  \ddots & 0 \\
0 & 0 & \ldots & 1\end{array} \right)$$
\noindent
If $[T]{\alpha}^{\beta}$ is the identity matrix $I{n \times n}$, then it means that $T$ sends each element of the basis $\alpha$ to the corresponding element of the basis $\beta$. In other words, $T(\alpha_i) = \beta_i$ for $1\leq i \leq n$, where $\alpha_i$ and $\beta_i$ denote the $i$-th elements of the bases $\alpha$ and $\beta$, respectively. This implies that $T$ is a bijection and $dim(V) = dim(W)$, since $T$ maps a basis of $V$ to a basis of $W$. Moreover, since $T$ is the identity map on each element of the basis $\alpha$, it follows that $T$ is also the identity map on $V$, i.e., $T(v) = v$ for all $v \in V$. Therefore, $T$ is both injective and surjective, and hence an isomorphism.
\newpage

\noindent
\textcolor{cyan}{Let $T:V \to W$ be a linear transformation between finite dimensional vector spaces $V$ with bases $\alpha$ and $\alpha^{\prime}$ and $W$ with bases $\beta$ and $\beta^{\prime}$. Let $I_V: V \to V$ and $I_W: W \to W$ be the respective identity transformations.}\\
\\
\textcolor{cyan}{Theorem 2.7.5 tells us that $$[T]_{\alpha^{\prime}}^{\beta^{\prime}} = [I_W]_{\beta}^{\beta^{\prime}} \cdot [T]_{\alpha}^{\beta} \cdot [I_V]_{\alpha^{\prime}}^{\alpha}$$}

\bigskip

\noindent
\textcolor{cyan}{Explain which transformation each of those matrices represents. Also, explain why the result of $$[I_W]_{\beta}^{\beta^{\prime}} [T]_{\alpha}^{\beta} [I_V]_{\alpha^{\prime}}^{\alpha} [v]_{\alpha^{\prime}}$$ }

\bigskip

\noindent
\textcolor{cyan}{is written in terms of basis elements of $\beta^{\prime}$.}

\noindent
In the expression $[T]{\alpha^{\prime}}^{\beta^{\prime}} = [I_W]{\beta}^{\beta^{\prime}} \cdot [T]{\alpha}^{\beta} \cdot [I_V]{\alpha^{\prime}}^{\alpha}$, we have:

\begin{itemize}
	\item $[I_W]_{\beta}^{\beta^{\prime}}$ is the matrix representing the identity transformation $I_W$ with respect to the bases $\beta$ and $\beta^{\prime}$ of $W$.
	\item $[T]_{\alpha}^{\beta}$ is the matrix representing the linear transformation $T$ with respect to the bases $\alpha$ and $\beta$ of $V$ and $W$ respectively.
	\item $[I_V]_{\alpha^{\prime}}^{\alpha}$ is the matrix representing the identity transformation $I_V$ with respect to the bases $\alpha^{\prime}$ and $\alpha}$ of $V$.
\end{itemize}

The result of [I_W]_{\beta}^{\beta^{\prime}} [T]_{\alpha}^{\beta} [I_V]_{\alpha^{\prime}}^{\alpha} [v]_{\alpha^{\prime}} is a vector in $W$ that is obtained by the following steps:

\begin{itemize}
	\item Starting with the vector $[v]{\alpha^{\prime}}$ in $V$, we use the matrix $[I_V]{\alpha^{\prime}}^{\alpha}$ to express it as a linear combination of the basis vectors of $\alpha$.
	\item We then apply the linear transformation $T$ to this linear combination of basis vectors to obtain a linear combination of the basis vectors of $\beta$.
	\item Finally, we use the matrix $[I_W]_{\beta}^{\beta^{\prime}}$ to express this linear combination of basis vectors of $\beta$ as a vector in $W$ with respect to the basis $\beta^{\prime}$.
\end{itemize}

\noindent
Since the output of this expression is a vector in $W$ with respect to the basis $\beta^{\prime}$, it makes sense that the coefficients of the linear combination are written in terms of the basis elements of $\beta^{\prime}$.

\newpage

\noindent
\textcolor{cyan}{Give an example two bases $\alpha$ and $\alpha^{\prime}$ for $P_2(\mathbb{R})$ and a polynomial $v \in P_2(\mathbb{R})$ so that $[v]_{\alpha}$ and $[v]_{\alpha^{\prime}}$ have different coordinates.}\\
\\
\textcolor{cyan}{Also find the change of basis matrix $[I]_{\alpha}^{\alpha^{\prime}}$ so that }

\textcolor{cyan}{$$[I]_{\alpha}^{\alpha^{\prime}} [v]_{\alpha} = [v]_{\alpha^{\prime}}$$}
\noindent
Let $\alpha = {1, x, x^2}$ be the standard basis for $P_2(\mathbb{R})$ and let $\alpha^{\prime} = {1, (x-1), (x-1)^2}$ be another basis for $P_2(\mathbb{R})$. Consider the polynomial $v = 2x^2 + 3x + 1 \in P_2(\mathbb{R})$.

\noindent
To find $[v]_{\alpha}$, we express $v$ as a linear combination of the elements of $\alpha$:

$$v = 1(1) + 3(x) + 2(x^2) $$

\noindent
So we have $[v]_{\alpha} = \begin{bmatrix} 1 \ 3 \ 2 \end{bmatrix}$.

\noindent
To find $[v]_{\alpha^{\prime}}$, we first express $\alpha$ in terms of $\alpha^{\prime}$:
$$1 = (x-1) + 2((x - 1)^2) $$
$$x = (x-1) + (x-1)^2 $$
$$x^2  = (x-1)^2 + 2(x-1)$$

\noindent
Thus, we have:

\begin{align*}
v &= 1(1) + 3(x) + 2(x^2) \\
&= 1[(x-1)+2((x-1)^2)] + 3[(x-1)+(x-1)^2] + 2[(x-1)^2+2(x-1)] \\
&= 5(x-1)^2 + 7(x-1) + 2
\end{align*}

\noindent
So we have $[v]_{\alpha^{\prime}} = \begin{bmatrix} 2 \\ 7 \\ 5 \end{bmatrix}$.

\noindent
To find the change of basis matrix $[I]_{\alpha}^{\alpha^{\prime}}$, we need to express each element of $\alpha$ in terms of $\alpha^{\prime}$. Using the equations we found earlier, we have:

$$[1]_{\alpha^{\prime}} = \begin{bmatrix} 1 \\ -1 \\ 1 \end{bmatrix},\ [(x - 1)]_{\alpha^{\prime}} = \begin{bmatrix} 0 \\ 1 \\ -2 \end{bmatrix}, \ [(x - 1)^2]_{\alpha^{\prime}} = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$$

\noindent
So the change of basis matrix is:

$$[I]_{\alpha}^{\alpha^{\prime}} = \begin{bmatrix} 1 & 0 & 0 \\ -1 & 0 & 0 \\ 1  & -2 & 1 \end{bmatrix} $$

\end{document}

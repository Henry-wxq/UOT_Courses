\documentclass[fontsize=12pt]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage{latexsym, amsfonts, fullpage, lscape, cancel, array, lastpage}
\usepackage{mathrsfs, graphicx, amssymb, amsmath, amscd, amsthm, MnSymbol}
\usepackage{titlesec}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{float}
\usepackage{appendix}
\usepackage{color}
\usepackage{amsmath}
\usepackage{pifont}
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{amssymb}

\titleformat{\section}
  {\normalfont\Large\bfseries}{\S\thesection}{1em}{}

\usepackage[english]{babel}
\usepackage{eucal}

\title{Unit 2(b) Lecture Notes for MAT224}
\author{Xuanqi Wei 1009353209}

\date{31 January-2 February, 2023}

\begin{document}

\maketitle

\newpage

\section{2.2 Linear Transformations Between Finite Dimensional Vector Spaces Continued...}

\noindent
\textcolor{cyan}{We know that the product $[T]_{\alpha}^{\beta}e_i = [T(\alpha_i)]_{\beta}$ where $\alpha_i$ is the $i^{th}$ basis element in $\alpha$ and the subscript $\beta$ means ``as a linear combination of elements in basis $\beta$''.}\\
\\
\textcolor{cyan}{Note that $e_i$ is the vector with 1 in the $i^{th}$ entry and zero everywhere else.}\\
\\
\textcolor{cyan}{Now write $\vec{v}$ as a linear combination of standard basis elements $e_1, e_2, \ldots , e_n$.}\\
\\
\textcolor{cyan}{Explain why the product $[T]_{\alpha}^{\beta}[\vec{v}]$ is equal to a linear combination of vectors in basis $\beta$. What do the $e_i$'s represent here?}

Suppose $\vec{v}$ can be written as a linear combination of the standard basis vectors $e_1, e_2, \ldots , e_n$ as follows:

$$v = c_1e_1+ c_2e_2 + \ldots + c_ne_n $$

\noindent
where $c_1, c_2, \ldots, c_n$ are scalars.

\noindent
Then, we can apply the formula $[T]_{\alpha}^{\beta}e_i = [T(\alpha_i)]{\beta}$ to each basis vector $e_i$ to obtain:

$$[T]_{\alpha}^{\beta}e_1 = [T(\alpha_1)]{\beta}$$
$$[T]_{\alpha}^{\beta}e_2 = [T(\alpha_2)]{\beta}$$
$$ \cdot \cdot \cdot $$
$$[T]_{\alpha}^{\beta}e_n = [T(\alpha_n)]{\beta}$$

Using the linearity of $T$, we can write:

\begin{align*}
[T]_{\alpha}^{\beta}\vec{v} &= [T]_{\alpha}^{\beta}(c_1e_1 + c_2e_2 + \cdots + c_ne_n)\\
&= c_1[T]_{\alpha}^{\beta}e_1 + c_2[T]_{\alpha}^{\beta}e_2 + \cdots + c_n[T]_{\alpha}^{\beta}e_n\\
&= c_1[T(\alpha_1)]_{\beta} + c_2[T(\alpha_2)]_{\beta} + \cdots + c_n[T(\alpha_n)]_{\beta}
\end{align*}

\noindent
Thus, $[T]_{\alpha}^{\beta}\vec{v}$ is a linear combination of the vectors $[T(\alpha_1)]_{\beta}, [T(\alpha_2)]_{\beta}, \ldots, [T(\alpha_n)]_{\beta}$ in basis $\beta$.

\noindent
The $e_i$'s represent the standard basis vectors of the vector space, which can be used to write any vector in the space as a linear combination. In this context, $e_i$'s are used to write $\vec{v}$ as a linear combination of standard basis vectors, which is then used to express $[T]_{\alpha}^{\beta}\vec{v}$ as a linear combination of vectors in basis $\beta$.

\newpage

\section{2.3 Kernel and Image}

\noindent
\textcolor{cyan}{Define the set Ker(T) for a linear transformation $T: V \to W$.}

\noindent
The kernel, or null space, of a linear transformation $T: V \rightarrow W$ is the set of all vectors in the domain $V$ that map to the zero vector in the range $W$.

\noindent
In other words, the kernel of $T$, denoted as Ker$(T)$, is the set of all vectors $\vec{v} \in V$ such that $T(\vec{v}) = \vec{0}$, where $\vec{0}$ is the zero vector in $W$.

\noindent
Formally, we can write: $Ker(T) = \{v \in V: T(V)=0 \}$
\\

\noindent
\textcolor{cyan}{Prove that Ker(T) is a subspace of $V$. Notice that elements in Ker(T) are in $V$ by definition.}

\noindent
To prove that the kernel of a linear transformation $T: V \rightarrow W$, denoted as Ker$(T)$, is a subspace of $V$, we need to show that it satisfies the three properties of a subspace:

\begin{enumerate}
	\item The zero vector is in Ker$(T)$. By definition, the zero vector in $V$ is mapped to the zero vector in $W$ by any linear transformation. Therefore, $\vec{0} \in$ Ker$(T)$. 
	\item Ker$(T)$ is closed under vector addition. Let $\vec{u}, \vec{v} \in$ Ker$(T)$. This means that $T(\vec{u}) = \vec{0}$ and $T(\vec{v}) = \vec{0}$. We need to show that $\vec{u}+\vec{v} \in$ Ker$(T)$. To do so, we evaluate $T(\vec{u}+\vec{v})$ as follows:
$$T(u + v) = T(u) + T(V) = 0 + 0 = 0 $$
$$\text{Therefore, $\vec{u}+\vec{v} \in$ Ker$(T)$, and Ker$(T)$ is closed under vector addition.} $$
	\item Ker$(T)$ is closed under scalar multiplication. Let $\vec{v} \in$ Ker$(T)$ and let $k$ be a scalar. This means that $T(\vec{v}) = \vec{0}$. We need to show that $k\vec{v} \in$ Ker$(T)$. To do so, we evaluate $T(k\vec{v})$ as follows:
$$T(kv) = kT(v) = k0 = 0 $$
\end{enumerate}

Therefore, $k\vec{v} \in$ Ker$(T)$, and Ker$(T)$ is closed under scalar multiplication.

Since Ker$(T)$ satisfies all three properties of a subspace, it is indeed a subspace of $V$.
\\

\noindent
\textcolor{cyan}{Define the set Image(T) for a linear transformation $T: V \to W$.}

\noindent
The image, or range, of a linear transformation $T: V \rightarrow W$ is the set of all vectors in the range $W$ that can be obtained as the output of $T$ when the input is a vector in the domain $V$.

\noindent
In other words, the image of $T$, denoted as Im$(T)$, is the set of all vectors $\vec{w} \in W$ that can be expressed as $\vec{w} = T(\vec{v})$ for some $\vec{v} \in V$.

\noindent
Formally, we can write:
$$Im(T) = \{w \in W: w = T(V)\ for\ some\ v\in V \} $$

\noindent
\textcolor{cyan}{Prove that Image(T) is a subspace of $W$. Notice that elements in Image(T) are in $W$ by definition.}


\begin{enumerate}
	\item The zero vector is in Im$(T)$. By definition, $T(\vec{0}) = \vec{0}$ for any linear transformation $T$, so $\vec{0} \in$ Im$(T)$.
	\item Im$(T)$ is closed under vector addition. Let $\vec{w}_1, \vec{w}_2 \in$ Im$(T)$. This means that there exist vectors $\vec{v}_1, \vec{v}_2 \in V$ such that $T(\vec{v}_1) = \vec{w}_1$ and $T(\vec{v}_2) = \vec{w}_2$. We need to show that $\vec{w}_1 + \vec{w}_2 \in$ Im$(T)$. To do so, we evaluate $T(\vec{v}_1 + \vec{v}_2)$ as follows:
$$T(v_1+ v_2) = T(v_1) + T(v_2) = w_1 + w_2 $$
$$\text{Therefore, $\vec{w}_1 + \vec{w}_2 \in$ Im$(T)$, and Im$(T)$ is closed under vector addition.} $$
	\item Im$(T)$ is closed under scalar multiplication. Let $\vec{w} \in$ Im$(T)$ and let $k$ be a scalar. This means that there exists a vector $\vec{v} \in V$ such that $T(\vec{v}) = \vec{w}$. We need to show that $k\vec{w} \in$ Im$(T)$. To do so, we evaluate $T(k\vec{v})$ as follows:
$$T(kv) = kT(v) = kw $$
$$\text{Therefore, $k\vec{w} \in$ Im$(T)$, and Im$(T)$ is closed under scalar multiplication.} $$
\end{enumerate}
Since Im$(T)$ satisfies all three properties of a subspace, it is indeed a subspace of $W$.
\\

\noindent
\textcolor{cyan}{Is it possible to determine the pre-image of a vector $\vec{w} \in W$, if you know its image? In other words, let $T: V \to W$ be a linear transformation and let $\vec{w} \in T(W)$. So $T(v) = w$ for some $v \in V$. Can we tell which $v \in V$ is sent to $W$ by V.\\}

\noindent
It is not always possible to determine the pre-image of a vector $\vec{w} \in W$ if you only know its image under a linear transformation $T: V \rightarrow W$.

\noindent
Consider a simple example where $T: \mathbb{R}^2 \rightarrow \mathbb{R}$ is defined by $T(x,y) = x$. Let $\vec{w} = 1 \in \mathbb{R}$. We know that $T(1,0) = 1$, so $(1,0)$ is one possible pre-image of $\vec{w}$. However, $T(0,1) = 0$ and $(0,1)$ is also a valid pre-image of $\vec{w}$.

\noindent
In this example, we can see that the pre-image of $\vec{w}$ is not unique. This is because the kernel of $T$ (i.e., the set of all vectors that are mapped to the zero vector in $W$) is non-trivial, and so there are multiple vectors in $V$ that get mapped to the same vector in $W$.

\noindent
\textcolor{cyan}{What are some of the things you can do with the matrix form of T (in other words $[T]_{\alpha}^{\beta})$ to learn about the properties of $T$?}

The matrix form of a linear transformation $T$ with respect to bases $\alpha$ and $\beta$ (i.e., $[T]_{\alpha}^{\beta}$) contains useful information about the properties of $T$. Here are some things you can do with the matrix form of $T$:

\begin{enumerate}
	\item Determine if $T$ is injective: The linear transformation $T$ is injective if and only if the kernel of $T$ is trivial, i.e., $\text{Ker}(T) = {\vec{0}}$. The matrix form of $T$ can help us determine if $\text{Ker}(T)$ is trivial by checking if the matrix is invertible.
	\item Determine if $T$ is surjective: The linear transformation $T$ is surjective if and only if the image of $T$ is equal to the entire range space $W$. The matrix form of $T$ can help us determine if $T$ is surjective by checking if every element of $W$ can be written as a linear combination of the columns of $[T]_{\alpha}^{\beta}$.
	\item Find the rank of $T$: The rank of $T$ is the dimension of the image of $T$. The rank of $T$ can be determined by finding the number of linearly independent columns in $[T]_{\alpha}^{\beta}$.
	\item Find the nullity of $T$: The nullity of $T$ is the dimension of the kernel of $T$. The nullity of $T$ can be determined by finding the number of free variables in the row reduced form of $[T]_{\alpha}^{\beta}$.
	\item Find the eigenvalues and eigenvectors of $T$: The matrix form of $T$ can be used to find the eigenvalues and eigenvectors of $T$ by solving the characteristic equation $\det([T]_{\alpha}^{\beta} - \lambda I) = 0$.
	\item Determine if $T$ is diagonalizable: The linear transformation $T$ is diagonalizable if and only if there exists a basis of $V$ consisting of eigenvectors of $T$. The matrix form of $T$ can help us determine if $T$ is diagonalizable by checking if there are enough linearly independent eigenvectors to form a basis of $V$.
\end{enumerate}

\noindent
\textcolor{cyan}{For a linear transformation $T: V \to W$, how can you use two of the following quantities to determine the third: \[ dim(Ker(T)) + dim(Im(T)) = dim(V)\]}

We can use the formula dim(Ker(T)) + dim(Im(T)) = dim(V) to determine any one of the three quantities if we know the other two. We just need to rearrange it. 

\newpage

\section{2.4 Application of the Dimension Theorem}

\bigskip

\noindent
\textcolor{cyan}{Give examples of deciding if a linear transformation $T: V \to W$ is injective or not based on:}
\begin{itemize}
  \item its kernel: a linear transformation $T: V \to W$ is injective if and only if its kernel is trivial, that is, if $ker(T) = {\vec{0}_V}$. Let $T: \mathbb{R}^2 \to \mathbb{R}^3$ be the linear transformation defined by $T(x,y) = (x,y,0)$. The kernel of $T$ is the set of all vectors of the form $(x,y)$ such that $T(x,y) = (0,0,0)$, which implies $x=y=0$. Therefore, $ker(T) = {(0,0)}$, which is the trivial subspace of $\mathbb{R}^2$. Since the kernel is trivial, $T$ is injective.
  \item its image: a linear transformation $T: V \to W$ is injective if and only if its image is a subspace of $W$ with the same dimension as the domain, that is, if $T$ is one-to-one, then $dim(Im(T)) = dim(V)$. Let $T: \mathbb{R}^2 \to \mathbb{R}^2$ be the linear transformation defined by $T(x,y) = (x,0)$. The image of $T$ is the $x$-axis in $\mathbb{R}^2$, which is a subspace of dimension $1$. Since the domain of $T$ is also $\mathbb{R}^2$, which has dimension $2$, $T$ cannot be injective.
  \item the relative dimensions of V and W: a linear transformation $T: V \to W$ is injective if and only if the kernel of $T$ is trivial, that is, if $dim(Ker(T))=0$. Let $T: \mathbb{R}^2 \to \mathbb{R}^3$ be the linear transformation defined by $T(x,y) = (x,y,0)$. Since the range of $T$ is a subspace of $\mathbb{R}^3$ that lies in the $xy$-plane, its dimension is $2$. Since the domain of $T$ is $\mathbb{R}^2$, which has dimension $2$, $T$ is injective if and only if the kernel of $T$ is trivial. But the kernel of $T$ consists of all vectors of the form $(0,0,z)$, which has dimension $1$, so $T$ is not injective.
  \item the relative dimensions of Im(T) and W: If $\operatorname{Im}(T)$ has the same dimension as $W$, then $T$ must be surjective (onto), but it may or may not be injective. $T: \mathbb{R}^3 \rightarrow \mathbb{R}^2$ defined by $T(x,y,z) = (x+y, z)$, i.e. the transformation that adds the first two coordinates and ignores the third. In this case, $\operatorname{Im}(T)$ is the plane $z=0$ in $\mathbb{R}^2$, which has lower dimension than $\mathbb{R}^2$, so $T$ cannot be injective.
  \end{itemize}

\newpage
\noindent
\textcolor{cyan}{Give the definition of ``surjective'' and explain how you can decide if a linear transformation $T$ is surjective based on the relative dimensions of $V$ and $W$.}

\noindent
A linear transformation $T: V \rightarrow W$ is said to be surjective (or onto) if every element of $W$ is the image of at least one element in $V$ under $T$, i.e., if $\operatorname{Im}(T) = W$.

\noindent
To determine whether a linear transformation $T: V \rightarrow W$ is surjective based on the relative dimensions of $V$ and $W$, we can use the following theorem:

\noindent
$T$ is surjective if and only if $\operatorname{dim}(\operatorname{Im}(T)) = \operatorname{dim}(W)$.
\\

\noindent
\textcolor{cyan}{Let $T:V-->W$ be a linear transformation where dim(V)=dim(W).}\\
\\
\textcolor{cyan}{(i) If $T$ is surjective, does it also have to be injective? Why or why not?}\\
\\ No, if $T:V\rightarrow W$ is a linear transformation where $\operatorname{dim}(V) = \operatorname{dim}(W)$ and $T$ is surjective, it does not have to be injective.

\noindent
Here is an example: Let $V = \mathbb{R}^2$ and $W = \mathbb{R}^2$, and let $T: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ be the linear transformation defined by $T(x,y) = (x,0)$. This transformation maps every vector in $V$ onto the $x$-axis in $W$, which covers all of $W$ and makes $T$ surjective. However, $T$ is not injective, since every vector in the $x$-axis in $W$ is the image of infinitely many vectors in $V$ (all vectors of the form $(x,y)$ where $y$ is any real number).

\noindent
Therefore, it is possible for a linear transformation $T:V\rightarrow W$ to be surjective without being injective, even when $\operatorname{dim}(V) = \operatorname{dim}(W)$.
\\

\textcolor{cyan}{(ii) If $T$ is injective, does it also have to be surjective? Why or why not?}

\noindent
If $T:V\rightarrow W$ is a linear transformation where $\operatorname{dim}(V) = \operatorname{dim}(W)$ and $T$ is injective, then $T$ is also surjective.

\noindent
This is because if $T$ is injective, then every element in the image of $T$ corresponds to exactly one element in $V$ (its pre-image). Since $\operatorname{dim}(V) = \operatorname{dim}(W)$, this means that $T$ is also surjective, since it maps all of the $n$-dimensional space $V$ onto all of the $n$-dimensional space $W$.

\vspace{3 in}

\noindent
\textcolor{cyan}{Consider Proposition 2.4.11. Let $\vec{w_0} \notin Ker(T)$ for some linear transformation $T: V \to W$. Then we know we can write every vector $\vec{v} \in V$ as $\vec{v} = \vec{w_0} + \vec{k}$ for some $\vec{k} \in Ker(T)$. If we replace $w_0$ with $w_1$ then does $k$ stay the same?}\\
\\
\textcolor{cyan}{In other words, $\vec{v} = \vec{w_1} + \vec{k}$ for the same $\vec{k}$ as before or $\vec{v} = \vec{w_1} + \vec{k^{\prime}}$ for a potentially different $\vec{k^{\prime}} \in Ker(T)$?}

\noindent
No, the vector $\vec{k}$ may change. If we replace $\vec{w_0}$ with $\vec{w_1}$, then we have $\vec{v} = \vec{w_1} + \vec{k'}$ for some $\vec{k'} \in Ker(T)$. It is possible that $\vec{k'} \neq \vec{k}$. To see why, consider a simple example where $V=W=\mathbb{R}^2$, and let $T$ be the linear transformation that projects onto the $x$-axis. Then $Ker(T)$ consists of all vectors of the form $(0, y)$, and any vector of the form $(x, y)$ can be written as $(x, y) = (x, 0) + (0, y)$, where $(x, 0)$ is a vector on the $x$-axis and $(0, y)$ is in $Ker(T)$. Now, if we replace $(1, 0)$ with $(0, 1)$, then any vector of the form $(x, y)$ can be written as $(0, 1) + (x, y-1)$, where $(0, 1)$ is not on the $x$-axis and $(x, y-1)$ is in $Ker(T)$. Thus, the vector $\vec{k}$ has changed from $(0, y)$ to $(x, y-1)$.


\end{document}
